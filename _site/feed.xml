<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-05-25T08:04:50-05:00</updated><id>http://localhost:4000/</id><title type="html">B K</title><subtitle>statistics, programming, philosophy</subtitle><entry><title type="html">Linear regression part four - Finding an unbiased estimate of the variance in the linear model.</title><link href="http://localhost:4000/regression/2018/05/23/simple-linear-regression-iv.html" rel="alternate" type="text/html" title="Linear regression part four - Finding an unbiased estimate of the variance in the linear model." /><published>2018-05-23T00:00:00-05:00</published><updated>2018-05-23T00:00:00-05:00</updated><id>http://localhost:4000/regression/2018/05/23/simple-linear-regression-iv</id><content type="html" xml:base="http://localhost:4000/regression/2018/05/23/simple-linear-regression-iv.html">&lt;p&gt;(Here, I borrow heavily from Christensen, &lt;em&gt;Plane Answers to Complex Questions&lt;/em&gt;.) Consider again the linear model&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(Y|X) = X\beta.&lt;/script&gt;

&lt;p&gt;Now, regard each column of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; as a vector in &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^n&lt;/script&gt;, and consider &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt; – the column space of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; – i.e. the subspace of &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^n&lt;/script&gt; spanned by the columns of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Define the perpendicular projection operator &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt; as follows: if &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; is in &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;Mv = v&lt;/script&gt;. And if &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; is in the space perpendicular to &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt; then &lt;script type=&quot;math/tex&quot;&gt;Mv = 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We need three lemmas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 1:&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;X(X'X)^{-1}X'&lt;/script&gt; is the perpendicular projection operator onto &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Let &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; be in &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt;. Then &lt;script type=&quot;math/tex&quot;&gt;v = Xb&lt;/script&gt; for some vector &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;. Hence&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X(X'X)^{-1}X'v = X(X'X)^{-1}X'Xb = Xb = v&lt;/script&gt;

&lt;p&gt;Now, let &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; be in the space perpendicular to &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt;. Then if &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is any column of &lt;script type=&quot;math/tex&quot;&gt;X, x'v = 0&lt;/script&gt;. Hence &lt;script type=&quot;math/tex&quot;&gt;X'v = 0&lt;/script&gt;, and therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X(X'X)^{-1}X'v = X(X'X)^{-1}0 = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tag*{$\Box$}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Lemma 2:&lt;/strong&gt; If &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; is the p.p.o. onto &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;tr(M) = r(X)&lt;/script&gt; – the trace of &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; is the rank of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Recall two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;Av = \lambda v&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is a square matrix, then &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; is an eigenvector of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is the associated eigenvalue. The multiplicity of the eigenvalue &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is the number of linearly independent vectors &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;Av = \lambda v,&lt;/script&gt; and the total number of eigenvalues of an &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; matrix is &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Any symmetric matrix &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; admits of a &lt;em&gt;singular value decomposition&lt;/em&gt;: &lt;script type=&quot;math/tex&quot;&gt;A=P'DP&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; is an orthogonal matrix (&lt;script type=&quot;math/tex&quot;&gt;P'P = I&lt;/script&gt;), and &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; is a diagonal matrix, with diagonal entries the eigenvalues of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, by definition, if &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; is in &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;Mv = v&lt;/script&gt;. Hence &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; is an eigenvalue of &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;, and because every column of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is in &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt;, the multiplicity of &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; is the number of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;’s columns that are linearly independent. In other words, the multiplicity of &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; is the rank of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Further, if &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; is in the space perpendicular to &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;Mv = 0v.&lt;/script&gt; So &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; is another eigenvalue of &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;, and its multiplicity is, similarly to the above, the dimension of the space perpendicular to &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt;. And that characterizes all of the eigenvalues of &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Note finally that &lt;script type=&quot;math/tex&quot;&gt;M = X'(X'X)^{-1}X&lt;/script&gt; is symmetric, so&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;tr(M) = tr(P'DP) = tr(DP'P) = \sum_i \lambda_i = \underbrace{1 + 1 + \ldots  + 1}_{r(X) \text{ times}} + 0 + 0 + \ldots + 0 = r(X)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\tag*{$\Box$}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 3:&lt;/strong&gt; if &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is a square matrix, and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is a random vector, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(Y'AY) = tr(A cov(Y)) + E(Y)'AE(Y)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;  First, recognize that where &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is any square matrix, &lt;script type=&quot;math/tex&quot;&gt;[Y - E(Y)]'A[Y - E(Y)]&lt;/script&gt; is a scalar. Hence &lt;script type=&quot;math/tex&quot;&gt;tr([Y - E(Y)]'A[Y - E(Y)]) = [Y - E(Y)]'A[Y - E(Y)]&lt;/script&gt;. Second, note&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	E[[Y - E(Y)]'A[Y - E(Y)]] &amp;= E[Y'AY - 2E(Y)'AY + E(Y)'AE(Y)] \\
	&amp;= E[Y'AY] - E(Y)'AE(Y)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;And hence&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	E[Y'AY] &amp;= E[[Y - E(Y)]'A[Y - E(Y)]] + E(Y)'AE(Y) \\
	&amp;= E[tr([Y - E(Y)]'A[Y - E(Y)])] + E(Y)'AE(Y) \\
	&amp;= E[tr(A[Y - E(Y)]'[Y - E(Y)])] + E(Y)'AE(Y) \\
	&amp;= tr(E[A[Y - E(Y)]'[Y - E(Y)]]) + E(Y)'AE(Y) \\
	&amp;= tr(A E[[Y - E(Y)]'[Y - E(Y)]] ) + E(Y)'AE(Y) \\
	&amp;= tr(A cov(Y)) + E(Y)'AE(Y)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tag*{$\Box$}&lt;/script&gt;

&lt;p&gt;That, finally, supplies all the resources we need to find an unbiased estimator of &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;. Recall our quadratic-loss-minimizing line &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta} = (X'X)^{-1}X'Y&lt;/script&gt;, and suppose that &lt;script type=&quot;math/tex&quot;&gt;E(Y) = X\beta&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;cov(Y) = \sigma^2 I&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now, consider&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;SSE = [Y - X\hat{\beta}]'[Y  - X \hat{\beta}]&lt;/script&gt;

&lt;p&gt;and notice&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	E(SSE) &amp;= E[Y'Y - 2\hat{\beta}'X'Y + \hat{\beta}'X'X\hat{\beta}] \\
	&amp;= E[Y'Y - 2Y'X(X'X)^{-1}X'Y + Y'X(X'X)^{-1}X'X(X'X)^{-1}X'Y] \\
	&amp;= E[Y'Y - Y'X(X'X)^{-1}X'Y] \\
	&amp;= E[Y'Y] - E[Y'X(X'X)^{-1}X'Y]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So, using &lt;strong&gt;Lemma 3&lt;/strong&gt; twice,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	E(SSE) &amp;= E[Y'Y] - E[Y'X(X'X)^{-1}X'Y] \\
	&amp;= [tr(cov(Y)) + \beta'X'X\beta] - [tr(X(X'X)^{-1}X' cov(Y)) + \beta'X'X(X'X)^{-1}X'X\beta] \\
	&amp;= n\sigma^2 + \beta'X'X\beta - [tr(X(X'X)^{-1}X'\sigma^2) + \beta'X'X\beta] \\
	&amp;= n\sigma^2 - tr(X(X'X)^{-1}X')\sigma^2
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;But by &lt;strong&gt;Lemma 1&lt;/strong&gt;, &lt;script type=&quot;math/tex&quot;&gt;X(X'X)^{-1}X'&lt;/script&gt; is the perpendicular projection operator onto &lt;script type=&quot;math/tex&quot;&gt;C(X)&lt;/script&gt;. Hence, by &lt;strong&gt;Lemma 2&lt;/strong&gt;, &lt;script type=&quot;math/tex&quot;&gt;tr(X(X'X)^{-1}X) = r(X)&lt;/script&gt;. If &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is non-singular, and of dimension &lt;script type=&quot;math/tex&quot;&gt;n \times p&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;r(X) = p&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Hence&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(SSE) = (n - p)\sigma^2&lt;/script&gt;

&lt;p&gt;Or in other words, &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{n -p} SSE&lt;/script&gt; is an unbiased estimator of &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is the rank of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">(Here, I borrow heavily from Christensen, Plane Answers to Complex Questions.) Consider again the linear model</summary></entry><entry><title type="html">Linear regression part three - if the mean is linear, the quadratic loss minimizing line is an unbiased estimate of that mean. A normal response yields normality in the quadratic loss minimizer.</title><link href="http://localhost:4000/regression/2018/05/11/simple-linear-regression-iii.html" rel="alternate" type="text/html" title="Linear regression part three - if the mean is linear, the quadratic loss minimizing line is an unbiased estimate of that mean. A normal response yields normality in the quadratic loss minimizer." /><published>2018-05-11T00:00:00-05:00</published><updated>2018-05-11T00:00:00-05:00</updated><id>http://localhost:4000/regression/2018/05/11/simple-linear-regression-iii</id><content type="html" xml:base="http://localhost:4000/regression/2018/05/11/simple-linear-regression-iii.html">&lt;p&gt;In the last two posts, I first found the line that minimizes quadratic loss over a fixed data set. That line, again, is given by the vector:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(X'X)^{-1}X'Y.&lt;/script&gt;

&lt;p&gt;We then regarded &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as a random object – drawn from some distribution &lt;script type=&quot;math/tex&quot;&gt;P(Y \vert X)&lt;/script&gt;, and showed that, if &lt;script type=&quot;math/tex&quot;&gt;cov(Y) = \sigma^2 I&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;cov[(X'X)^{-1}X'Y] = \sigma^2 (X'X)^{-1}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now, let us make additional assumptions.  Let us assume that the conditional mean &lt;script type=&quot;math/tex&quot;&gt;E(Y \vert X)&lt;/script&gt; is linear in &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, i.e. that &lt;script type=&quot;math/tex&quot;&gt;E(Y \vert X) = X\beta&lt;/script&gt;, where again:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
		Y = \begin{bmatrix}
			y_1 \\
			y_2 \\
			\vdots \\
			y_n
		\end{bmatrix}, \quad
		X = \begin{bmatrix}
			1 &amp; x_1 \\
			1 &amp; x_2 \\
			\vdots &amp; \vdots \\
			1 &amp; x_n
		\end{bmatrix}, \quad \text{and }
		\beta = \begin{bmatrix}
			\beta_0 \\
			\beta_1
		\end{bmatrix}
	\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Then notice that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	E[(X'X)^{-1}X'Y \vert X] &amp;= (X'X)^{-1}X'E(Y) \\
	&amp;= (X'X)^{-1}X'X\beta \\
	&amp;= \beta
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So if, in fact, the conditional mean of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; given &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is linear in &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;X(X'X)^{-1}X'Y&lt;/script&gt; is an unbiased estimate of that conditional mean.&lt;/p&gt;

&lt;p&gt;Further, if we assume that each of the &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt;’s is normally distributed around its respective mean. Then because any linear combination of independent normally distributed random variables is itself normally distributed, &lt;script type=&quot;math/tex&quot;&gt;(X'X)^{-1}X'Y&lt;/script&gt; is also normally distributed; and hence by our earlier results&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\beta} = (X'X)^{-1}X'Y \sim N(\beta, \sigma^2(X'X)^{-1})&lt;/script&gt;</content><author><name></name></author><summary type="html">In the last two posts, I first found the line that minimizes quadratic loss over a fixed data set. That line, again, is given by the vector:</summary></entry><entry><title type="html">Linear regression part two - finding the covariance of the quadratic loss minimizing line under homoskedasticity.</title><link href="http://localhost:4000/regression/2018/04/26/simple-linear-regression-ii.html" rel="alternate" type="text/html" title="Linear regression part two - finding the covariance of the quadratic loss minimizing line under homoskedasticity." /><published>2018-04-26T00:00:00-05:00</published><updated>2018-04-26T00:00:00-05:00</updated><id>http://localhost:4000/regression/2018/04/26/simple-linear-regression-ii</id><content type="html" xml:base="http://localhost:4000/regression/2018/04/26/simple-linear-regression-ii.html">&lt;p&gt;Last time, I began with a graph.&lt;/p&gt;

&lt;p&gt;Namely, this one:
&lt;img src=&quot;/images/no_line.png&quot; alt=&quot;Scatter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And we regarded the points on that graph as fixed, and found the linear function of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; which minimized quadratic loss for those points. Recall we assumed nothing stochastic; we were just minimizing a loss function taken over fixed data. And recall also that if we let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
		Y = \begin{bmatrix}
			y_1 \\
			y_2 \\
			\vdots \\
			y_n
		\end{bmatrix}, \quad \text{and} \quad
		X = \begin{bmatrix}
			1 &amp; x_1 \\
			1 &amp; x_2 \\
			\vdots &amp; \vdots \\
			1 &amp; x_n
		\end{bmatrix}
	\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;then we found that the vector&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\beta} = (X'X)^{-1}X'Y&lt;/script&gt;

&lt;p&gt;supplies the line which minimizes quadratic loss for our data. Its first element is the intercept, and its second is the slope.&lt;/p&gt;

&lt;p&gt;But typically, we do not think of our data as fixed. Instead, we see each data point as a realization from a chance process – a realization from a joint probability distribution, &lt;script type=&quot;math/tex&quot;&gt;P(x, y)&lt;/script&gt;, governing &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;. (We might, for example, think of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; as weight, and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; as height, and then imagine selecting someone at random. &lt;script type=&quot;math/tex&quot;&gt;P(x,y)&lt;/script&gt; would then describe the probability of selecting a person of weight &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and height &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;And if we do see our data as realizations from a chance process, then we might wonder: if we had seen &lt;em&gt;alternative&lt;/em&gt; realizations from &lt;script type=&quot;math/tex&quot;&gt;P(x,y)&lt;/script&gt;, how would we expect our quadratic-loss-minimizing line to have differed? The below picture, for example, shows two 100 data point samples from the same underlying distribution on &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, along with the quadratic-loss-minimizing lines for each:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/two_samp_lines.png&quot; alt=&quot;TwoSamplesWithLine&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Appreciate how they shift and vary.&lt;/p&gt;

&lt;p&gt;Now, we might &lt;em&gt;also&lt;/em&gt; wonder: if we regard our &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-values as &lt;em&gt;fixed&lt;/em&gt;, how would we expect our quadratic-loss-minimizing line to have varied? That is, how would we expect them to vary given alternative realizations from &lt;script type=&quot;math/tex&quot;&gt;P(y \vert x)&lt;/script&gt;?&lt;/p&gt;

&lt;p&gt;The below picture shows 20 100-data point samples from the same distribution &lt;script type=&quot;math/tex&quot;&gt;P(y \vert x)&lt;/script&gt;, along with quadratic-loss-minimizing lines for each:
&lt;img src=&quot;/images/lin_fit_lines.png&quot; alt=&quot;Twenty Lines&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Appreciate, again, how they shift and vary. Notice also the vertical striations in the data on the graph. These are a result of the fact that the vector of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-values is &lt;em&gt;fixed&lt;/em&gt;, and I’ve sampled &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; from around those fixed &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-values.&lt;/p&gt;

&lt;p&gt;The difference between the first question – ‘how would I expect the lines to vary if I saw alternative realizations from &lt;script type=&quot;math/tex&quot;&gt;P(x,y)&lt;/script&gt;?’ – and the second question – ‘how would I expect the lines to vary if I saw alternative realizations from &lt;script type=&quot;math/tex&quot;&gt;P(y \vert x)&lt;/script&gt;?’ – is subtle. The first asks – across samples from &lt;script type=&quot;math/tex&quot;&gt;P(x,y)&lt;/script&gt;, how do quadratic-loss-minimizers tend to vary. The second asks – across all samples from &lt;script type=&quot;math/tex&quot;&gt;P(x,y)&lt;/script&gt; &lt;em&gt;where the vector of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; values is equal to the vector of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-values I happened to witness&lt;/em&gt; – how do quadratic-loss-minimizers tend to vary? Here, we’ll focus on this latter question, mostly because it’s easier to answer, but also because it’s more relevant, given we saw the data we did.&lt;/p&gt;

&lt;p&gt;Now, without knowing the distribution &lt;script type=&quot;math/tex&quot;&gt;P(y \vert x)&lt;/script&gt; it’s hard to say much about the variation we should expect in our quadratic-loss-minimizing lines. But we can make the problem tractable by imposing some additional constraints.&lt;/p&gt;

&lt;p&gt;For example, assume that, conditional on the vector of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-values, each response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is independent of every other response &lt;script type=&quot;math/tex&quot;&gt;y_j&lt;/script&gt;.  Assume also that – again conditional on the vector of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-values – the variance of the responses &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is constant, and denote that constant variance by ‘&lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;’. In other words, assume that for the vector &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; values, &lt;script type=&quot;math/tex&quot;&gt;cov(Y \vert X) = \sigma^2 I&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Given that assumption, we can compute the covariance matrix of &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	cov (\hat{\beta} | X) &amp;= E[(\hat{\beta} - E(\hat{\beta}))(\hat{\beta} - E(\hat{\beta}))' | X] \\
	&amp;= E\big[\big(X'X)^{-1}X'Y - E((X'X)^{-1}X'Y)\big) \big((X'X)^{-1}X'Y - E((X'X)^{-1}X'Y)\big)' | X] \\
	&amp;= E[(X'X)^{-1}X'YY'X(X'X)^{-1} - \\
	&amp; \quad \qquad (X'X)^{-1}X'E(Y)Y'X(X'X)^{-1} - \\
	&amp; \quad \qquad (X'X)^{-1}X'YE(Y)'X(X'X)^{-1} + \\ 
	&amp; \quad \qquad (X'X)^{-1}X'E(Y)E(Y)'X(X'X)^{-1} | X] \\
	&amp;= (X'X)^{-1}X'E\big[YY' - E(Y)Y' - Y'E(Y)' + E(Y)E(Y)' | X \big]X(X'X)^{-1} \\
	&amp;= (X'X)^{-1}X'\underbrace{E\big[(Y - E(Y))(Y - E(Y))' | X]}_{cov (Y | X)}X(X'X)^{-1} \\
	&amp;= (X'X)^{-1}X'\sigma^2IX(X'X)^{-1} \\
	&amp;= \sigma^2 (X'X)^{-1}X'X(X'X)^{-1} \\
	&amp;= \sigma^2 (X'X)^{-1}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;I think the generality of this result somewhat surprising, because it only relies on conditional independence and constant variance. Consider the below picture. It shows 20 100-data-point samples from a fixed-conditional-variance distribution, where &lt;script type=&quot;math/tex&quot;&gt;E(y \vert x) = x^3&lt;/script&gt;, along with 20 corresponding quadratic-loss-minimizing lines:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/cube_fit_lines.png&quot; alt=&quot;Cube mean&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, compare that picture to a similar picture below – with the same &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-values, but where the &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;-values are generated from a distribution with &lt;script type=&quot;math/tex&quot;&gt;E(y \vert x) = x&lt;/script&gt; – i.e., where the conditional mean of &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is a linear, instead of a cubic, function of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/lin_fit_lines_cube_scale.png&quot; alt=&quot;linear mean&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see generally different slopes across the two plots. That’s not surprising. But consider: both plots are on the same scale, and the variation in the quadratic-loss-minimizing lines is (approximately) the same in both cases. This, despite the fact that the conditional mean of &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is a much more complicated function of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; in the former case than in the latter.&lt;/p&gt;

&lt;p&gt;I think that last &lt;em&gt;is&lt;/em&gt; surprising. More precisely, I think it surprising that the covariance matrix of &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; – which expresses the amount of variation in our quadratic-loss-minimizing lines – is independent of &lt;script type=&quot;math/tex&quot;&gt;E(y \vert x)&lt;/script&gt; considered as a function of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. Under every such function the expected variation in our quadratic-loss-minimizing lines is the same.&lt;/p&gt;

&lt;p&gt;That’s enough for now; something that is also surprising is that there is still more to say about drawing straight lines through a crop of data points. So, still more to come on this.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Code below: generate 20 100-data-point samples from the same conditional distribution, and plot quadratic-loss-minimizers for each.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Init structures to store samples&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta_hat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Generate some x-values. X[,2] ~ N(0, 1)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncol&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Over 20 iterations, generate 100 y-values, Y ~ N(X[,2], 1), &lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## and compute quadratic-loss-minimizers&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbind.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cbind.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  					      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  					      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Find (X'X)^(-1)X'Y.&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bh&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta_hat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbind.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                               &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cbind.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                               &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                               &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Plot the samples along with their quadratic-loss-minimizers.&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theme_minimal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scale_color_grey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Sample'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theme&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_abline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
              &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name></name></author><summary type="html">Last time, I began with a graph.</summary></entry><entry><title type="html">Linear regression part one - linear quadratic loss minimizer.</title><link href="http://localhost:4000/regression/2018/04/22/simple-linear-regression.html" rel="alternate" type="text/html" title="Linear regression part one - linear quadratic loss minimizer." /><published>2018-04-22T00:00:00-05:00</published><updated>2018-04-22T00:00:00-05:00</updated><id>http://localhost:4000/regression/2018/04/22/simple-linear-regression</id><content type="html" xml:base="http://localhost:4000/regression/2018/04/22/simple-linear-regression.html">&lt;p&gt;If there is one thing statisticians are good at, it is drawing straight lines. (Peter Mueller deserves the credit for that – he said it in a lecture once.)&lt;/p&gt;

&lt;p&gt;I write this partly to get the blog up and running, and partly because it’s good to begin at the beginning.&lt;/p&gt;

&lt;p&gt;Suppose you have a graph that looks like this: 
&lt;img src=&quot;/images/no_line.png&quot; alt=&quot;Scatter&quot; /&gt; 
And suppose you wanted to capture the data on that graph, as well as possible, using a linear function of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; – i.e., something of the form &lt;script type=&quot;math/tex&quot;&gt;\beta_0 + \beta_1 x&lt;/script&gt;. You might imagine that you would like a friend, who has access to all of the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; values of the points above, to recover the corresponding &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; values, but that you only have enough bandwidth to transmit a slope and an intercept.&lt;/p&gt;

&lt;p&gt;Now, suppose further that the cost you pay for any error in your friend’s estimate is proportional to the squared error between her guess - based on the slope and intercept you provided – and the true &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; value.&lt;/p&gt;

&lt;p&gt;In that situation, the “best” line is the one that minimizes quadratic loss – the sum of the squared vertical distances between each point and the line in question. So you want to find &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; which satisfy:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
	argmin_{\beta_0, \beta_1} \sum_i (y_i - (\beta_0 + \beta_1x_i))^2
\end{align*}&lt;/script&gt;

&lt;p&gt;Equivalently, if we let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
		Y = \begin{bmatrix}
			y_1 \\
			y_2 \\
			\vdots \\
			y_n
		\end{bmatrix}, \quad
		X = \begin{bmatrix}
			1 &amp; x_1 \\
			1 &amp; x_2 \\
			\vdots &amp; \vdots \\
			1 &amp; x_n
		\end{bmatrix}, \quad \text{and }
		\beta = \begin{bmatrix}
			\beta_0 \\
			\beta_1
		\end{bmatrix}
	\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;then the best line is the one that minimizes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;argmin_\beta (Y - X\beta)'(Y - X\beta).&lt;/script&gt;

&lt;p&gt;To find that minimum, first notice that because &lt;script type=&quot;math/tex&quot;&gt;\beta'X'Y&lt;/script&gt; is a scalar, and hence because &lt;script type=&quot;math/tex&quot;&gt;(\beta'X'Y)' = Y'X\beta = \beta'X'Y&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	(Y - X\beta)'(Y - X\beta) &amp;=  Y'Y - \beta'X'Y - Y'X\beta + \beta'X'X\beta \\ &amp;= Y'Y  - 2\beta'X'Y + \beta'X'X\beta
	\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So, to find the &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; which minimizes quadratic loss, take partials of that last, set them to zero, and solve:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\frac{\partial}{\partial \beta} (Y - X\beta)'(Y - X\beta) = -2X'Y + 2X'X\beta = 0 \\
&amp;\Rightarrow X'X\beta = X'Y \\
&amp;\Rightarrow \beta = (X'X)^{-1}X'Y
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The last step is kosher so long as &lt;script type=&quot;math/tex&quot;&gt;X'X&lt;/script&gt; is invertible.&lt;/p&gt;

&lt;p&gt;Regarding the data in the scatterplot above as fixed, the contour plot below shows the value of the quadratic loss function, &lt;script type=&quot;math/tex&quot;&gt;\sum_i (y_i - (\beta_0 + \beta_1x_i))^2 = (Y - X \beta)'(Y - X\beta)&lt;/script&gt; for differing values of &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt;. The tiny ‘x’ marks the unique minimum we just found, &lt;script type=&quot;math/tex&quot;&gt;(X'X)^{-1}X'Y&lt;/script&gt;.
&lt;img src=&quot;/images/ql.png&quot; alt=&quot;Quadratic Loss&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the line through our points, which corresponds to that choice of &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt;, is drawn in the below picture.
&lt;img src=&quot;/images/graph.png&quot; alt=&quot;Points 'round a line&quot; /&gt;
Voila.&lt;/p&gt;

&lt;p&gt;Notice there was nothing stochastic in anything above - no assumptions about a probability distribution governing &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, or normally distributed epsilons or anything like that. If what you care about is quadratic loss, then &lt;script type=&quot;math/tex&quot;&gt;\beta = (X'X)^{-1}X'Y&lt;/script&gt; gives you the best line through your points, probability distributions be damned.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Some R code below. Simulate some data, then find and plot the line which minimizes quadratic loss for that data.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Simulate some data. X[,2] ~ N(0, 1), Y ~ N(X[,2], 1).&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncol&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Find (X'X)^(-1)X'Y.&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta_hat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Plot the data, with the regression line.&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cbind.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fun&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theme_minimal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theme&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name></name></author><summary type="html">If there is one thing statisticians are good at, it is drawing straight lines. (Peter Mueller deserves the credit for that – he said it in a lecture once.)</summary></entry></feed>